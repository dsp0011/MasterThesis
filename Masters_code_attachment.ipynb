{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6g4jEw9XmBGt"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UevMI4xMohrb"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import h5py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn import tree\n",
        "import random\n",
        "from collections import Counter\n",
        "import os\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjgeJQj9xjDC"
      },
      "source": [
        "# Feature validation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mev_features =[ 'sumMT', 'met', 'meff', 'ht', \"jet_1_pt\", \"jet_1_mtMet\",  \"jet_2_pt\", \"jet_2_mtMet\",  \"tau_1_pt\", \"tau_1_mtMet\" ]\n",
        "\n",
        "b_weights   = pd.concat(\n",
        "    [background[0]['lumiweight']*background[0]['mcEventWeight']*background[0]['pileupweight'],\n",
        "     background[1]['lumiweight']*background[1]['mcEventWeight']*background[1]['pileupweight'],\n",
        "     background[2]['lumiweight']*background[2]['mcEventWeight']*background[2]['pileupweight'],\n",
        "     background[3]['lumiweight']*background[3]['mcEventWeight']*background[3]['pileupweight'],\n",
        "     background[4]['lumiweight']*background[4]['mcEventWeight']*background[4]['pileupweight'],\n",
        "     background[5]['lumiweight']*background[5]['mcEventWeight']*background[5]['pileupweight'],\n",
        "     background[6]['lumiweight']*background[6]['mcEventWeight']*background[6]['pileupweight'],\n",
        "     background[7]['lumiweight']*background[7]['mcEventWeight']*background[7]['pileupweight'],\n",
        "     background[8]['lumiweight']*background[8]['mcEventWeight']*background[8]['pileupweight'],\n",
        "     background[9]['lumiweight']*background[9]['mcEventWeight']*background[9]['pileupweight'],\n",
        "     background[10]['lumiweight']*background[10]['mcEventWeight']*background[10]['pileupweight']\n",
        "    ])\n",
        "\n",
        "data_weights = np.array(data['lumiweight']*data['mcEventWeight']*data['pileupweight'])\n",
        "signal_weights = np.array(signal['lumiweight']*signal['mcEventWeight']*signal['pileupweight'])"
      ],
      "metadata": {
        "id": "211YvKq_JxF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoy-qiDcuu0m"
      },
      "outputs": [],
      "source": [
        "\n",
        "#\n",
        "#\n",
        "#    Some of the code genereating feature validation plots has been \n",
        "#    inspired by a previous master thesis by Ã˜ivind Birkeland.\n",
        "#    https://drive.google.com/drive/folders/176ZulAphX8f10QznOJWATfc2Icssu5Zl   \n",
        "#\n",
        "#\n",
        "\n",
        "\n",
        "def plot_validation(feature_name, x_start, x_end, bins, weighted = True, limit = True): \n",
        "    concat_background = pd.concat (\n",
        "        [background[0][feature_name],\n",
        "         background[1][feature_name],\n",
        "         background[2][feature_name],\n",
        "         background[3][feature_name],\n",
        "         background[4][feature_name],\n",
        "         background[5][feature_name],\n",
        "         background[6][feature_name],\n",
        "         background[7][feature_name],\n",
        "         background[8][feature_name],\n",
        "         background[9][feature_name],\n",
        "         background[10][feature_name]\n",
        "        ]\n",
        "    )\n",
        "\n",
        "\n",
        "    b_weights   = pd.concat(\n",
        "      [background[0]['lumiweight']*background[0]['mcEventWeight']*background[0]['pileupweight'],\n",
        "      background[1]['lumiweight']*background[1]['mcEventWeight']*background[1]['pileupweight'],\n",
        "      background[2]['lumiweight']*background[2]['mcEventWeight']*background[2]['pileupweight'],\n",
        "      background[3]['lumiweight']*background[3]['mcEventWeight']*background[3]['pileupweight'],\n",
        "      background[4]['lumiweight']*background[4]['mcEventWeight']*background[4]['pileupweight'],\n",
        "      background[5]['lumiweight']*background[5]['mcEventWeight']*background[5]['pileupweight'],\n",
        "      background[6]['lumiweight']*background[6]['mcEventWeight']*background[6]['pileupweight'],\n",
        "      background[7]['lumiweight']*background[7]['mcEventWeight']*background[7]['pileupweight'],\n",
        "      background[8]['lumiweight']*background[8]['mcEventWeight']*background[8]['pileupweight'],\n",
        "      background[9]['lumiweight']*background[9]['mcEventWeight']*background[9]['pileupweight'],\n",
        "      background[10]['lumiweight']*background[10]['mcEventWeight']*background[10]['pileupweight']\n",
        "      ])\n",
        "\n",
        "    background_full = [background[0][feature_name], background[1][feature_name], background[2][feature_name], background[3][feature_name],\n",
        "          background[4][feature_name], background[5][feature_name], background[6][feature_name], background[7][feature_name],\n",
        "          background[8][feature_name], background[9][feature_name], background[10][feature_name]]\n",
        "\n",
        "\n",
        "    bins = np.linspace(data[feature_name].min(),data[feature_name].max(), bins)\n",
        "    bin_centres = (bins[:-1] + bins[1:])/2\n",
        "    colors = ['royalblue', 'dodgerblue', 'steelblue', 'skyblue', 'firebrick', 'indianred', 'salmon', 'forestgreen', 'darkgreen', 'mediumseagreen', 'darkslategrey']\n",
        "    colors = ['#003f5c', '#2f4b7c', '#665191', '#a05195', '#d45087', '#f95d6a', '#ff7c43', '#ffa600', '#D9B000', '#CCB311', '#99BF22']\n",
        "\n",
        "\n",
        "    mc_total_hist = np.histogram(concat_background, bins, weights=b_weights)[0]\n",
        "    mc_total_err = np.sqrt(np.histogram(concat_background, bins, weights=b_weights**2)[0])\n",
        "    data_hist = np.histogram(data[feature_name], bins, weights=data_weights)[0]\n",
        "    data_err = np.sqrt(np.histogram(data[feature_name], bins, weights=data_weights**2)[0])\n",
        "    data_mc = data_hist / mc_total_hist \n",
        "    data_mc_err = np.sqrt( (data_err**2 / data_hist**2 + mc_total_err**2 / mc_total_hist**2) * data_mc**2 )\n",
        "\n",
        "\n",
        "    plt.clf()\n",
        "    fig = plt.figure(figsize=(16, 9))\n",
        "    ax_1, ax_2 = fig.subplots(2, 1, gridspec_kw={'height_ratios': [4, 1]})\n",
        "\n",
        "    if limit:\n",
        "      ax_1.set_xlim(x_start, x_end)\n",
        "    ax_1.set_yscale('log')\n",
        "    ax_1.set_ylim(1e-1, 1e6)\n",
        "    ax_1.set_ylabel('Events per bin', fontsize=11)\n",
        "    ax_1.ticklabel_format(axis='x', style='plain')\n",
        "    if feature_name in mev_features: ax_1.set_xlabel(feature_name + \" [MeV]\" , fontsize=14, labelpad=5)\n",
        "    else: ax_1.set_xlabel(feature_name, fontsize=14, labelpad=5)\n",
        "\n",
        "\n",
        "\n",
        "    #MC sets are stacked and plotted\n",
        "    ax_1.hist(\n",
        "        background_full,\n",
        "        weights=b_weights,\n",
        "        bins=bins,\n",
        "        color=colors,\n",
        "        stacked=True,\n",
        "        label=backgroundLabel\n",
        "        )\n",
        "    \n",
        "    #MC error is plotted\n",
        "    ax_1.fill_between(\n",
        "        bin_centres,\n",
        "        mc_total_hist+mc_total_err,\n",
        "        mc_total_hist-mc_total_err,\n",
        "        step=\"mid\",\n",
        "        alpha=0.4,\n",
        "        color=\"gray\",\n",
        "        zorder=100, \n",
        "        \n",
        "        label='MC error'   \n",
        "    )\n",
        "\n",
        "    #Signal is plotted // wieght differently if using all datasets?\n",
        "    ax_1.hist(\n",
        "        signal[feature_name],\n",
        "        weights=signal_weights,\n",
        "        bins=bins,\n",
        "        color='firebrick',\n",
        "        histtype=u'step',\n",
        "        linewidth=1.2,\n",
        "        label='Signal'\n",
        "        )\n",
        "\n",
        "    #Data is plotted as points with errorbars\n",
        "    ax_1.errorbar(\n",
        "        bin_centres,\n",
        "        data_hist,\n",
        "        yerr=data_err,\n",
        "\n",
        "        color='black',\n",
        "        fmt='o',\n",
        "        label='data'\n",
        "    )\n",
        "\n",
        "    ax_1.legend(\n",
        "      ncol=2,\n",
        "      edgecolor=\"black\",\n",
        "      title_fontsize= 13,\n",
        "      frameon=True,\n",
        "      fancybox=True,\n",
        "      framealpha=0.4,\n",
        "      loc='best'\n",
        "      )\n",
        "\n",
        "\n",
        "    ax_2.set_ylim(0.5, 1.5)\n",
        "    ax_2.set_yticks([0.50, 0.75, 1.00, 1.25, 1.50])\n",
        "    ax_2.ticklabel_format(axis='x', style='plain')\n",
        "    ax_2.set_ylabel('Data / Monte Carlo', fontsize=11)\n",
        "\n",
        "    #The data/MC is plotted with errorbars\n",
        "    ax_2.axhline(y=1.0, color='#2f4b7c', linestyle='-')\n",
        "    ax_2.errorbar(bin_centres, data_mc, yerr=data_mc_err, color='black', fmt='o')\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01QSIp_Zrz8-"
      },
      "source": [
        "# Custom decision tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZB0D6HN74w3A"
      },
      "source": [
        "## Implementation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U70zEyT1Oe-u"
      },
      "source": [
        "### imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHSZa_DlOe-v"
      },
      "outputs": [],
      "source": [
        "import pkg_resources\n",
        "pkg_resources.require(\"numpy==1.19.5\")\n",
        "import numpy\n",
        "import os\n",
        "from numba import jit\n",
        "from statsmodels.stats.weightstats import DescrStatsW\n",
        "import tkinter as tk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHxYQdpfOe-v"
      },
      "source": [
        "### GPU functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3Kt5JVhOe-v"
      },
      "outputs": [],
      "source": [
        "#\n",
        "#\n",
        "#     The following section includes function that are GPU optimized\n",
        "#\n",
        "#\n",
        "\n",
        "@jit(nopython= True, fastmath=True, parallel= True)\n",
        "def _custom_split_GPU(parent, parent_weights, child, child_weights):\n",
        "  parent_background_count = 0\n",
        "  parent_signal_count = 0\n",
        "\n",
        "  unweighted_parent_background_count = 0\n",
        "  unweighted_parent_signal_count = 0\n",
        "  \n",
        "\n",
        "  for i in range(len(parent)):\n",
        "    if parent[i] == 1:\n",
        "      parent_signal_count += parent_weights[i]\n",
        "      unweighted_parent_signal_count += parent[i]\n",
        "    else:\n",
        "      parent_background_count += parent_weights[i]\n",
        "      unweighted_parent_background_count += parent[i]\n",
        "\n",
        "  child_background_count = 0\n",
        "  child_signal_count = 0\n",
        "\n",
        "  for i in range(len(child)):\n",
        "    if child[i] == 1:\n",
        "      child_signal_count += child_weights[i]\n",
        "    else:\n",
        "      child_background_count += child_weights[i]\n",
        "  try:\n",
        "    measure_children = child_signal_count / math.sqrt( child_signal_count + child_background_count)\n",
        "    measure_parent = parent_signal_count / math.sqrt( parent_signal_count + parent_background_count)\n",
        "\n",
        "    # Alternative loss function using Asimov significance\n",
        "    # s = child_signal_count\n",
        "    # b = child_background_count\n",
        "    # measure_children = math.sqrt(2 * ( (s + b) * math.log(1 + (s/b) )  - s ) )\n",
        "\n",
        "    s = parent_signal_count\n",
        "    b = parent_background_count\n",
        "    measure_parent = math.sqrt(2 * ( (s + b) * math.log(1 + (s/b) )  - s ) )\n",
        "  except:\n",
        "    return 0, 0,0, 0, 0, 0 ,0\n",
        "\n",
        "  measure = measure_children - measure_parent \n",
        "  return measure, 0, 0, unweighted_parent_signal_count, unweighted_parent_background_count, parent_signal_count, parent_background_count\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "@jit(nopython= True, fastmath=True)\n",
        "def _custom_split_GPU_sum(parent, parent_weights, child, child_weights, s_s, s_b):\n",
        "\n",
        "  child_background_count_sum = s_b\n",
        "  child_signal_count_sum = s_s\n",
        "\n",
        "  signal_count, background_count, weighted_signal_count, weighted_background_count = 0, 0, 0, 0\n",
        "  parent_signal_count, parent_background_count, parent_weighted_signal_count, parent_weighted_background_count = 0, 0, 0, 0\n",
        "\n",
        "\n",
        "  for i in range(len(parent)):\n",
        "    if parent[i] == 1:\n",
        "      parent_weighted_signal_count += parent_weights[i]\n",
        "      parent_signal_count += 1\n",
        "    else:\n",
        "      parent_weighted_background_count += parent_weights[i]\n",
        "      parent_background_count += 1\n",
        "\n",
        "\n",
        "  for i in range(len(child)):\n",
        "    if child[i] == 1:\n",
        "      weighted_signal_count += child_weights[i]\n",
        "      signal_count += child[i]\n",
        "    else:\n",
        "      weighted_background_count += child_weights[i]\n",
        "      background_count += child[i]\n",
        "\n",
        "  try:\n",
        "    child_signal_count_sum += weighted_signal_count\n",
        "    child_background_count_sum += weighted_background_count\n",
        "\n",
        "    measure_child =  child_signal_count_sum / math.sqrt( child_signal_count_sum + child_background_count_sum)\n",
        "\n",
        "    # Alternative loss function using Asimov significance\n",
        "    # s = child_signal_count_sum\n",
        "    # b = child_background_count_sum\n",
        "    # measure_child = math.sqrt(2 * ( (s + b) * math.log(1 + (s/b) )  - s ) )\n",
        "\n",
        "    if s_s == 0 and s_b == 0:\n",
        "      measure_parent = 0\n",
        "    else:\n",
        "      # Alternative loss function using Asimov significance\n",
        "      # s = s_s\n",
        "      # b = s_b\n",
        "      # measure_parent = math.sqrt(2 * ( (s + b) * math.log(1 + (s/b) )  - s ) )\n",
        "\n",
        "      measure_parent = s_s / math.sqrt( s_s + s_b)\n",
        "\n",
        "    measure = measure_child - measure_parent\n",
        "\n",
        "\n",
        "    return measure, child_signal_count_sum, child_background_count_sum, parent_signal_count, parent_background_count, parent_weighted_signal_count, parent_weighted_background_count\n",
        "  except:\n",
        "    return 0, 0, 0, 0, 0, 0, 0\n",
        "\n",
        "\n",
        "@jit(nopython=True)\n",
        "def _variance_reduction(parent, l_child, r_child):\n",
        "    weight_l = len(l_child) / len(parent)\n",
        "    weight_r = len(r_child) / len(parent)\n",
        "    reduction = np.var(parent) - (weight_l * np.var(l_child) + weight_r * np.var(r_child))\n",
        "    return reduction\n",
        "\n",
        "\n",
        "@jit(nopython=True)\n",
        "def _calc_var(arr, arr_weights, mean):\n",
        "    sum = 0\n",
        "    count = 0 \n",
        "    \n",
        "    for i in range(0,len(arr)):\n",
        "        sum +=  arr_weights[i]*(arr[i] - mean) ** 2\n",
        "        count += arr_weights[i]\n",
        "    try:\n",
        "      return sum / count\n",
        "    except: \n",
        "      return 0\n",
        "\n",
        "@jit(nopython=True)\n",
        "def _get_avg(arr, arr_weights):\n",
        "    sum = 0 \n",
        "    count = 0\n",
        "    for i in range(0,len(arr)):\n",
        "        sum += (arr[i] * arr_weights[i])\n",
        "        count += arr_weights[i]\n",
        "    if count ==0:\n",
        "      return 0\n",
        "    return sum / count\n",
        "\n",
        "@jit(nopython=True)\n",
        "def _sum(arr):\n",
        "  sum=0\n",
        "  for i in range(len(arr)):\n",
        "    sum+= arr[i]\n",
        "  return sum\n",
        "\n",
        "@jit(nopython=True)\n",
        "def _weighted_variance_reduction(l_child, l_weights, r_child, r_weights):\n",
        "\n",
        "  try:\n",
        "\n",
        "    l_mean_w = _get_avg(l_child, l_weights)\n",
        "    l_var = _calc_var(l_child, l_weights, l_mean_w)\n",
        "\n",
        "    r_mean_w = _get_avg(r_child, r_weights)\n",
        "    r_var = _calc_var(r_child, r_weights, r_mean_w)\n",
        "\n",
        "    p = np.append(l_child, r_child, axis=0)\n",
        "    p_weights = np.append(l_weights, r_weights, axis=0)\n",
        "\n",
        "    parent_background_count = 0\n",
        "    parent_signal_count = 0\n",
        "\n",
        "    parent_unweighted_background_count = 0\n",
        "    parent_unweighted_signal_count = 0\n",
        "\n",
        "\n",
        "    for i in range(len(p)):\n",
        "      if p[i] == 1:\n",
        "        parent_signal_count += p_weights[i]\n",
        "        parent_unweighted_signal_count +=1\n",
        "      else:\n",
        "       parent_background_count += p_weights[i]\n",
        "       parent_unweighted_background_count +=1\n",
        "\n",
        "    p_mean_w = _get_avg(p, p_weights)\n",
        "    p_var = _calc_var(p, p_weights, p_mean_w)\n",
        "\n",
        "    if _sum(p_weights) > 0:\n",
        "      l_size_weight = _sum(l_weights) / _sum(p_weights)\n",
        "      r_size_weight = _sum(r_weights) / _sum(p_weights)\n",
        "    else:\n",
        "      l_size_weight = 0\n",
        "      r_size_weight: 0\n",
        "\n",
        "    reduction = p_var - (l_size_weight * l_var + r_size_weight * r_var)\n",
        "    return reduction, parent_unweighted_signal_count, parent_unweighted_background_count, parent_signal_count, parent_background_count\n",
        "  except:\n",
        "    return 0, 0, 0, 0, 0\n",
        "\n",
        "\n",
        "\n",
        "@jit(nopython= True, fastmath=True, parallel= True)\n",
        "def _calculate_sensitivity(preds, target, weights):\n",
        "    s_s = 0\n",
        "    b_s = 0\n",
        "    for i in range(0, len(target)) :\n",
        "        if target[i] == 1 and preds[i] == 1:\n",
        "            s_s += weights[i]\n",
        "        if target[i] == 0 and preds[i] == 1:\n",
        "            b_s += weights[i]\n",
        "\n",
        "    return s_s, b_s, (s_s / math.sqrt(( b_s + s_s )))\n",
        "\n",
        "        \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKtnJZwNOe-w"
      },
      "source": [
        "### Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3qlrUJ7Vi4v"
      },
      "outputs": [],
      "source": [
        "#\n",
        "#\n",
        "#     The code responsible for generating decision trees is partially based on following material\n",
        "#     https://github.com/Suji04/ML_from_Scratch/blob/master/decision%20tree%20regression.ipynb\n",
        "#     \n",
        "#     An in-depth description of the functions is provided in Section 6.3\n",
        "#\n",
        "#\n",
        "\n",
        "\n",
        "class Node():\n",
        "    def __init__(self, feature=None, threshold=None, nth_chosen = None, left=None, right=None, var_red=None,\n",
        "                  s_s = None, b_b = None, w_s_s = None,  w_b_b = None, value=None):\n",
        "        \n",
        "        self.feature = feature\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.var_red = var_red\n",
        "        self.nth_chosen = nth_chosen\n",
        "\n",
        "\n",
        "        self.value = value\n",
        "\n",
        "\n",
        "\n",
        "        self.s_s = s_s\n",
        "        self.b_b = b_b\n",
        "\n",
        "        self.w_s_s = w_s_s\n",
        "        self.w_b_b = w_b_b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8noLOCdVi4v"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DecisionTreeRegressor():\n",
        "    def __init__(self, min_samples_split=1000, max_depth=2, features=[], weights=[], use_sum=False, split_type=\"custom\", model_type=\"regr\", min_split_gain=0.0):\n",
        "\n",
        "        # initialize the root of the tree\n",
        "        self.root = None\n",
        "\n",
        "        # stopping conditions\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.max_depth = max_depth\n",
        "        self.features = features\n",
        "        self.use_sum = use_sum\n",
        "        self.split_type = split_type\n",
        "        self.min_split_gain = min_split_gain\n",
        "        self.leaf_nodes = 0\n",
        "        self.tree_str = \"\"\n",
        "        self.model_type = model_type\n",
        "        self.feature_importance_dict = {}\n",
        "\n",
        "    def get_feature_importance_dict(self):\n",
        "        return self.feature_importance_dict\n",
        "\n",
        "    def add_to_feature_importance(self, feature, value):\n",
        "        new_feature_name = feature.replace(\"_\", \"\\n\")\n",
        "        if new_feature_name in self.feature_importance_dict:\n",
        "            self.feature_importance_dict[new_feature_name] += value\n",
        "        else:\n",
        "            self.feature_importance_dict[new_feature_name] = value\n",
        "\n",
        "    def build_tree(self, dataset, curr_depth=0, s_s=0, s_b=0, left=\"root\"):\n",
        "        num_samples = len(dataset)\n",
        "        weighted_num_samples = dataset['summed_weight'].sum()\n",
        "\n",
        "        best_split = {}\n",
        "        if weighted_num_samples >= self.min_samples_split and curr_depth <= self.max_depth:\n",
        "            best_split, s_s, s_b = self.get_best_split(\n",
        "                dataset, num_samples, s_s, s_b)\n",
        "\n",
        "            if not not best_split:\n",
        "                if best_split[\"var_red\"] > self.min_split_gain:\n",
        "                    self.add_to_feature_importance(\n",
        "                        best_split['feature'], best_split['var_red'])\n",
        "                    # recur right\n",
        "                    right_subtree = self.build_tree(\n",
        "                        best_split[\"dataset_right\"], curr_depth+1, s_s, s_b, left=\"right\")\n",
        "                    # recur left\n",
        "                    left_subtree = self.build_tree(\n",
        "                        best_split[\"dataset_left\"], curr_depth+1, s_s, s_b, left=\"left\")\n",
        "                    # return decision node\n",
        "                    return Node(best_split[\"feature\"], best_split[\"threshold\"], best_split[\"nth_chosen\"],\n",
        "                                left_subtree, right_subtree, best_split[\"var_red\"], best_split[\"s_s\"],\n",
        "                                best_split[\"b_b\"], best_split[\"w_s_s\"], best_split[\"w_b_b\"])\n",
        "\n",
        "        leaf_value = _get_avg(dataset['data_type'].to_numpy(), dataset['summed_weight'].to_numpy(\n",
        "        )) if self.model_type == \"regr\" else self.calculate_classification_leaf_value(left)\n",
        "\n",
        "        self.leaf_nodes += 1\n",
        "\n",
        "        leaf_weighted_background_count = 0\n",
        "        leaf_weighted_signal_count = 0\n",
        "\n",
        "        for i in range(0, len(dataset)):\n",
        "            if dataset['data_type'].values[i] == 1:\n",
        "                leaf_weighted_signal_count += dataset['summed_weight'].values[i]\n",
        "            else:\n",
        "                leaf_weighted_background_count += dataset['summed_weight'].values[i]\n",
        "\n",
        "        return Node(value=leaf_value, w_s_s=leaf_weighted_signal_count, w_b_b=leaf_weighted_background_count)\n",
        "\n",
        "    def calculate_sensitivity(self, X, target, weights):\n",
        "        preds = self.predict(X)\n",
        "        return _calculate_sensitivity(preds, target, weights)\n",
        "\n",
        "    def get_num_leaf_nodes(self):\n",
        "        return self.leaf_nodes\n",
        "\n",
        "    def calculate_classification_leaf_value(self, left):\n",
        "        return 1 if left == \"right\" else 0\n",
        "\n",
        "    def calculate_leaf_value(self, Y):\n",
        "        val = np.mean(Y)\n",
        "        return val\n",
        "\n",
        "    def print_tree(self, tree=None, indent=\"|\"):\n",
        "        tree_str = \"\"\n",
        "\n",
        "        if not tree:\n",
        "            tree = self.root\n",
        "\n",
        "        if tree.value is not None:\n",
        "            temp = \"\"\n",
        "            if type(tree.value) is tuple:\n",
        "                temp = str(tree.value[0]), \"w_s_s: \", str(\n",
        "                    round(tree.w_s_s, 3)), \"w_b_b: \", str(round(tree.w_b_b, 3))\n",
        "            else:\n",
        "                temp = str(tree.value), \"w_s_s: \", str(\n",
        "                    round(tree.w_s_s, 3)), \"w_b_b: \", str(round(tree.w_b_b, 3))\n",
        "            self.tree_str += str(temp)\n",
        "            print(temp)\n",
        "\n",
        "        else:\n",
        "            print(\"[\", str(tree.feature), \"<=\", str(round(tree.threshold, 2)), \"?\", str(round(tree.var_red, 2)), \"(\", str(tree.nth_chosen), \") \" + (\"s_s: \" + str(\n",
        "                round(tree.s_s, 2)) + \" | w_s_s: \" + str(round(tree.w_s_s, 2)) + \" | b_b:  \" + str(round(tree.b_b, 2)) + \" | w_b_b: \" + str(round(tree.w_b_b, 2)) + \" ]\"))\n",
        "            temp = \"[ \" + str(tree.feature) + \" \" + \"<=\" + \" \" + str(round(tree.threshold, 2)) + \" \" + \"?\" + \" \" + str(round(tree.var_red, 2)) + \" \" + \"(\" + \" \" + str(tree.nth_chosen) + \") ]\" + \\\n",
        "                \"s_s: \" + str(round(tree.s_s, 2)) + \" | w_s_s: \" + str(round(tree.w_s_s, 2)) + \\\n",
        "                \" | b_b:  \" + str(round(tree.b_b, 2)) + \\\n",
        "                \" | w_b_b: \" + str(round(tree.w_b_b, 2)) + \" ]\"\n",
        "            self.tree_str += temp\n",
        "\n",
        "            temp = \"\\n%sleft:\" % (indent)\n",
        "            self.tree_str += temp\n",
        "            print(\"|\")\n",
        "            print(\"%sleft:\" % (indent), end=\"\")\n",
        "            self.print_tree(tree.left, indent + \"--\")\n",
        "\n",
        "            temp = \"\\n%sright:\" % (indent)\n",
        "            self.tree_str += temp\n",
        "            print(\"|\")\n",
        "            print(\"%sright:\" % (indent), end=\"\")\n",
        "            self.print_tree(tree.right, indent + \"--\")\n",
        "\n",
        "    def return_tree_str(self):\n",
        "        return self.tree_str\n",
        "\n",
        "    def get_root(self):\n",
        "        return self.root\n",
        "\n",
        "    def fit(self, dataset):\n",
        "        self.root = self.build_tree(dataset)\n",
        "\n",
        "    def make_prediction_with_sens(self, x, tree, sens=-999):\n",
        "        if tree.value != None:\n",
        "            return (tree.value, sens)\n",
        "\n",
        "        # checks if the node is one before leaf node\n",
        "        if tree.left.value != None or tree.right.value != None:\n",
        "            signal_as_signal = tree.right.w_s_s\n",
        "            background_as_signal = tree.left.w_s_s\n",
        "\n",
        "            sens = 0\n",
        "            try:\n",
        "\t\t\t\t\t\t\t\tsens = signal_as_signal / math.sqrt(signal_as_signal + background_as_signal )\n",
        "            except:\n",
        "                sens = 0\n",
        "            feature_val = x[tree.feature]\n",
        "\n",
        "            if feature_val <= tree.threshold:\n",
        "                return self.make_prediction_with_sens(x, tree.left, sens)\n",
        "            else:\n",
        "                return self.make_prediction_with_sens(x, tree.right, sens)\n",
        "\n",
        "        else:\n",
        "\n",
        "            feature_val = x[tree.feature]\n",
        "\n",
        "            if feature_val <= tree.threshold:\n",
        "                return self.make_prediction_with_sens(x, tree.left)\n",
        "            else:\n",
        "                return self.make_prediction_with_sens(x, tree.right)\n",
        "\n",
        "    def make_prediction(self, x, tree):\n",
        "        if tree.value != None:\n",
        "            return tree.value\n",
        "        feature_val = x[tree.feature]\n",
        "        if feature_val <= tree.threshold:\n",
        "            return self.make_prediction(x, tree.left)\n",
        "        else:\n",
        "            return self.make_prediction(x, tree.right)\n",
        "\n",
        "    def predict(self, df, use_sens=False):\n",
        "        if not use_sens:\n",
        "            preds = df.apply(lambda row: self.make_prediction(\n",
        "                row, self.root), axis=1)\n",
        "            return preds.to_numpy()\n",
        "\n",
        "        preds = df.apply(lambda row: self.make_prediction_with_sens(\n",
        "            row, self.root), axis=1)\n",
        "        return zip(*preds)\n",
        "\n",
        "\n",
        "\t\t#\n",
        "\t\t#\n",
        "\t\t# \t\tCode for automatically generating margins between the x-ticks is partly based on the following link:\n",
        "\t\t#\t\t\thttps://stackoverflow.com/questions/44863375/how-to-change-spacing-between-ticks-in-matplotlib\n",
        "\t\t#\n",
        "\t\t#\n",
        "\n",
        "    def plot_feature_importance(self, save_path):\n",
        "        feature_importance = self.feature_importance_dict\n",
        "\n",
        "        features_sorted = dict(\n",
        "            sorted(feature_importance.items(), key=lambda item: item[1], reverse=True))\n",
        "        plt.clf()\n",
        "        plt.title(\"Decision tree feature importance\")\n",
        "\n",
        "        plt.bar(range(len(features_sorted)), list(\n",
        "            features_sorted.values()), align='center')\n",
        "        plt.xticks(range(len(features_sorted)), list(features_sorted.keys()))\n",
        "\n",
        "        N = len(self.features)\n",
        "        plt.xticks(range(N))  # add loads of ticks\n",
        "        plt.grid()\n",
        "\n",
        "        plt.gca().margins(x=0)\n",
        "        plt.gcf().canvas.draw()\n",
        "        tl = plt.gca().get_xticklabels()\n",
        "        maxsize = max([t.get_window_extent().width for t in tl])\n",
        "        m = 1.0  # inch margin\n",
        "        s = maxsize/plt.gcf().dpi*N+2*m\n",
        "        margin = m/plt.gcf().get_size_inches()[0]\n",
        "\n",
        "        plt.gcf().subplots_adjust(left=margin, right=1.-margin)\n",
        "        plt.gcf().set_size_inches(s, plt.gcf().get_size_inches()[1])\n",
        "\n",
        "        if save_path:\n",
        "            final_save_path = save_path + \"feature-importance.png\"\n",
        "            plt.savefig(final_save_path)\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    def get_best_split(self, dataset, num_samples, s_s, s_b):\n",
        "\n",
        "        best_split = {}\n",
        "        new_s_s, curr_s_s = 0, 0\n",
        "        new_s_b, curr_s_b = 0, 0\n",
        "        max_all_feature_split_measure = -float(\"inf\")\n",
        "        leaf_weighted_signal_cound, background_count, weighted_signal_count, weighted_background_count = 0, 0, 0, 0\n",
        "        counter = 0\n",
        "\n",
        "        for feature in self.features:\n",
        "            feature_values = dataset[feature]\n",
        "            all_thresholds = np.sort(\n",
        "                np.unique(feature_values.round(decimals=3)))\n",
        "            all_thresholds = self.get_reduced_threshold_list(\n",
        "                np.sort(all_thresholds))\n",
        "\n",
        "            counter = 0\n",
        "\n",
        "            for threshold in all_thresholds:\n",
        "                counter += 1\n",
        "                dataset_left, dataset_right = self.split(\n",
        "                    dataset, feature, threshold)\n",
        "                if len(dataset_left) > 0 and len(dataset_right) > 0:\n",
        "                    p, p_weights = dataset['data_type'].to_numpy(\n",
        "                    ), dataset['summed_weight'].to_numpy(),\n",
        "                    r_c, r_c_weights = dataset_right['data_type'].to_numpy(\n",
        "                    ), dataset_right['summed_weight'].to_numpy()\n",
        "                    l_c, l_c_weights = dataset_left['data_type'].to_numpy(\n",
        "                    ), dataset_left['summed_weight'].to_numpy()\n",
        "\n",
        "                    if self.split_type == \"custom\":\n",
        "                        current_feature_measure, curr_s_s, curr_s_b, signal_count, background_count, weighted_signal_count, weighted_background_count = self.custom_split_func_3(\n",
        "                            p, p_weights, r_c, r_c_weights, l_c, l_c_weights, s_s, s_b)\n",
        "                    else:\n",
        "                        current_feature_measure, signal_count, background_count, weighted_signal_count, weighted_background_count = self.variance_reduction(\n",
        "                            p, l_c, l_c_weights, r_c, r_c_weights)\n",
        "                        curr_s_s = 0\n",
        "                        curr_s_b = 0\n",
        "\n",
        "                    if current_feature_measure > max_all_feature_split_measure:\n",
        "\n",
        "                        best_split[\"feature\"] = feature\n",
        "                        best_split[\"threshold\"] = threshold\n",
        "                        best_split[\"dataset_left\"] = dataset_left\n",
        "                        best_split[\"dataset_right\"] = dataset_right\n",
        "                        best_split[\"var_red\"] = current_feature_measure\n",
        "                        best_split[\"nth_chosen\"] = counter\n",
        "                        best_split[\"s_s\"] = signal_count\n",
        "                        best_split[\"w_s_s\"] = weighted_signal_count\n",
        "                        best_split[\"b_b\"] = background_count\n",
        "                        best_split[\"w_b_b\"] = weighted_background_count\n",
        "                        max_all_feature_split_measure = current_feature_measure\n",
        "                        new_s_s = curr_s_s\n",
        "                        new_s_b = curr_s_b\n",
        "\n",
        "        return best_split, new_s_s, new_s_b\n",
        "\n",
        "    def custom_split_func(self, p, p_weights, r_c, r_c_weights, l_c, l_c_weights, s_s, s_b):\n",
        "        if not self.use_sum:\n",
        "            return _custom_split_GPU(p, p_weights, r_c, r_c_weights)\n",
        "        return _custom_split_GPU_sum(p, p_weights, r_c, r_c_weights, s_s, s_b)\n",
        "\n",
        "    def custom_split_func_2(self, p, p_weights, r_c, r_c_weights, l_c, l_c_weights, s_s, s_b):\n",
        "        return _custom_split_GPU_2(r_c, r_c_weights, l_c, l_c_weights)\n",
        "\n",
        "    def custom_split_func_3(self, p, p_weights, r_c, r_c_weights, l_c, l_c_weights, s_s, s_b):\n",
        "        return _custom_split_GPU_3(r_c, r_c_weights, l_c, l_c_weights)\n",
        "\n",
        "    def split(self, dataset, feature, threshold):\n",
        "        left = dataset[dataset[feature] <= threshold]\n",
        "        right = dataset[dataset[feature] > threshold]\n",
        "        return left, right\n",
        "\n",
        "    def get_reduced_threshold_list(self, possible_thresholds):\n",
        "        total = len(possible_thresholds)\n",
        "        reduced_list = []\n",
        "        for i in np.arange(0.0, 1.0, 0.02):\n",
        "            reduced_list.append(possible_thresholds[math.floor(total*i)])\n",
        "        return np.array(reduced_list)\n",
        "\n",
        "    def variance_reduction(self, parent, l_c, l_c_w, r_c, r_c_w):\n",
        "        return _weighted_variance_reduction(l_c, l_c_w, r_c, r_c_w)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFfxMfnVOe-y"
      },
      "source": [
        "## Random forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tql24v4EOe-z"
      },
      "source": [
        "### Prediction functions for feature validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3-rPHdhOe-z"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_validation(predicted_data, predicted_data_2, feature_name, x_start, x_end, bins, title, weighted = True, limit = False, save_path = \"\"): \n",
        "\n",
        "    signal = predicted_data.loc[predicted_data['data_type'] == 1]\n",
        "    signal_weights = signal['summed_weight']\n",
        "\n",
        "    signal_2 = predicted_data_2.loc[predicted_data_2['data_type'] == 1]\n",
        "    signal_2_weights = signal_2['summed_weight']\n",
        "\n",
        "    \n",
        "    background = [              \n",
        "                  df_test.loc[df_test['full_data_type'] == 2],\n",
        "                  df_test.loc[df_test['full_data_type'] == 3],\n",
        "                  df_test.loc[df_test['full_data_type'] == 4],\n",
        "                  df_test.loc[df_test['full_data_type'] == 5],\n",
        "                  df_test.loc[df_test['full_data_type'] == 6],\n",
        "                  df_test.loc[df_test['full_data_type'] == 7],\n",
        "                  df_test.loc[df_test['full_data_type'] == 8],\n",
        "                  df_test.loc[df_test['full_data_type'] == 9],\n",
        "                  df_test.loc[df_test['full_data_type'] == 10],\n",
        "                  df_test.loc[df_test['full_data_type'] == 11],\n",
        "                  df_test.loc[df_test['full_data_type'] == 12]\n",
        "              ]\n",
        "\n",
        "\n",
        "    x_weights = [\n",
        "            background[0]['summed_weight'],\n",
        "            background[1]['summed_weight'],\n",
        "            background[2]['summed_weight'],\n",
        "            background[3]['summed_weight'],\n",
        "            background[4]['summed_weight'],\n",
        "            background[5]['summed_weight'],\n",
        "            background[6]['summed_weight'],\n",
        "            background[7]['summed_weight'],\n",
        "            background[8]['summed_weight'],\n",
        "            background[9]['summed_weight'],\n",
        "            background[10]['summed_weight']\n",
        "            ]\n",
        "\n",
        "\n",
        "            \n",
        "\n",
        "    x = [background[0][feature_name], background[1][feature_name], background[2][feature_name], background[3][feature_name],\n",
        "          background[4][feature_name], background[5][feature_name], background[6][feature_name], background[7][feature_name],\n",
        "          background[8][feature_name], background[9][feature_name], background[10][feature_name]]\n",
        "\n",
        "\n",
        "\n",
        "    max_value= max(signal[feature_name].max(), signal_2[feature_name].max())\n",
        "    \n",
        "    for i in range(0, len(x)):\n",
        "      max_value = max(max_value, x[i].max())\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    bins = np.linspace(0, (max_value + max_value * 0.05))\n",
        "    bin_centres = (bins[:-1] + bins[1:])/2\n",
        "    colors = ['royalblue', 'dodgerblue', 'steelblue', 'skyblue', 'firebrick', 'indianred', 'salmon', 'forestgreen', 'darkgreen', 'mediumseagreen', 'darkslategrey']\n",
        "\n",
        "\n",
        "    plt.clf()\n",
        "    plt.figure(figsize=(16, 9))\n",
        "    ax_1  = plt.gca()\n",
        "\n",
        "    if limit:\n",
        "      ax_1.set_xlim(x_start, x_end)\n",
        "    ax_1.set_yscale('log')\n",
        "    ax_1.set_ylim(1e-1, 1e6)\n",
        "    ax_1.set_ylabel('Events per bin', fontsize=11)\n",
        "    ax_1.ticklabel_format(axis='x', style='plain')\n",
        "    ax_1.set_xlabel(feature_name + \" \" , fontsize=13)\n",
        "    ax_1.set_title(title)\n",
        "\n",
        "\n",
        "    ax_1.hist(\n",
        "        x,\n",
        "        weights=x_weights,\n",
        "        bins=bins,\n",
        "        color=colors,\n",
        "        stacked=True,\n",
        "        label=backgroundLabel\n",
        "        )\n",
        "    \n",
        "    ax_1.hist(\n",
        "        signal[feature_name],\n",
        "        weights=signal_weights,\n",
        "        bins=bins,\n",
        "        color='firebrick',\n",
        "        histtype=u'step',\n",
        "        linewidth=1.2,\n",
        "        label='Signal'\n",
        "        )\n",
        "\n",
        "\n",
        "    ax_1.legend(\n",
        "      ncol=2,\n",
        "      edgecolor=\"black\",\n",
        "      title_fontsize= 13,\n",
        "      frameon=True,\n",
        "      fancybox=True,\n",
        "      framealpha=0.4,\n",
        "      loc='best'\n",
        "      )\n",
        "\n",
        "    if save_path != \"\":\n",
        "      plt.savefig(save_path)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lx1glCo5Oe-z"
      },
      "outputs": [],
      "source": [
        "def classify_with_threshold(preds, threshold):\n",
        "  return [1 if pred > threshold else 0 for pred in preds]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bIq2zGcOe-z"
      },
      "outputs": [],
      "source": [
        "\n",
        "def predict_with_feature_validation(min_samples_split, max_depth, features, weights, df_train, df_test, save_path, n_estimators=60, train_frac=0.125, max_features=\"auto\"):\n",
        "    most_important_features = [\"jet_1_pt\", \"sumMT\", \"tau_1_mtMet\",\n",
        "                               \"METoverPtMean\", \"jet_2_mtMet\", \"ht\", \"tau_1_mtMet\", \"met\", \"meff\"]\n",
        "    # Variance reduction\n",
        "    print(\"------------------------VARIANCE REDUCTION-----------------------\")\n",
        "\n",
        "    var_red_save_path = save_path + \"-var-red-\"\n",
        "\n",
        "    regressor_var_red = RandomForest(min_samples_split, max_depth, features, weights,\n",
        "                                      False, \"var_red\", n_estimators, model_type=\"regr\", max_features=\"auto\")\n",
        "    regressor_var_red.fit(df_train, train_frac)\n",
        "    regressor_var_red.plot_feature_importance(save_path=var_red_save_path)\n",
        "    regressor_var_red.get_tree_info(save_path=var_red_save_path)\n",
        "\n",
        "    var_red_preds = regressor_var_red.predict(df_test)\n",
        "\n",
        "    regressor_var_red_max_sens_threshold = plot_sensitivity(\n",
        "        df_test, var_red_preds, np.array([]), max_depth, min_samples_split, var_red_save_path)\n",
        "    regressor_var_red_classification = classify_with_threshold(\n",
        "        var_red_preds, regressor_var_red_max_sens_threshold)\n",
        "\n",
        "    df_test['predicted_data_type'] = regressor_var_red_classification\n",
        "\n",
        "    predicted_as_signal = df_test.loc[df_test['predicted_data_type'] == 1]\n",
        "    predicted_as_background = df_test.loc[df_test['predicted_data_type'] == 0]\n",
        "\n",
        "    for feature in most_important_features:\n",
        "        plot_validation(predicted_as_signal, predicted_as_background,  feature, 18e4, 30e4, 300,\n",
        "                        title=\"var-red signal prediction\", limit=False, save_path=var_red_save_path + feature + \"-signal\")\n",
        "        plot_validation(predicted_as_background, predicted_as_signal, feature, 18e4, 30e4, 300,\n",
        "                        title=\"var-red background prediction\", limit=False, save_path=var_red_save_path + feature + \"-background\")\n",
        "\n",
        "    # Non-sum custom loss funcion\n",
        "\n",
        "    print(\"------------------------NON SUM -----------------------\")\n",
        "\n",
        "    non_sum_save_path = save_path + \"-non-sum-\"\n",
        "\n",
        "    regressor_non_sum = RandomForest(min_samples_split, max_depth, features, weights,\n",
        "                                      False, \"custom_1\", n_estimators, model_type=\"regr\", max_features=\"auto\")\n",
        "    regressor_non_sum.fit(df_train, train_frac)\n",
        "    regressor_non_sum.plot_feature_importance(save_path=non_sum_save_path)\n",
        "    regressor_non_sum.get_tree_info(save_path=non_sum_save_path)\n",
        "\n",
        "    non_sum_preds = regressor_non_sum.predict(df_test)\n",
        "\n",
        "    regressor_non_sum_max_sens_threshold = plot_sensitivity(\n",
        "        df_test, non_sum_preds, np.array([]), max_depth, min_samples_split, non_sum_save_path)\n",
        "    regressor_non_sum_classification = classify_with_threshold(\n",
        "        non_sum_preds, regressor_non_sum_max_sens_threshold)\n",
        "\n",
        "    df_test['predicted_data_type'] = regressor_non_sum_classification\n",
        "\n",
        "    predicted_as_signal = df_test.loc[df_test['predicted_data_type'] == 1]\n",
        "    predicted_as_background = df_test.loc[df_test['predicted_data_type'] == 0]\n",
        "\n",
        "    for feature in most_important_features:\n",
        "        plot_validation(predicted_as_signal, predicted_as_background, feature, 18e4, 30e4, 300,\n",
        "                        title=\"non-sum signal prediction\", limit=False,  save_path=non_sum_save_path + feature + \"-signal\")\n",
        "        plot_validation(predicted_as_background, predicted_as_signal, feature, 18e4, 30e4, 300,\n",
        "                        title=\"non-sum background prediction\", limit=False,  save_path=non_sum_save_path + feature + \"-background\")\n",
        "\n",
        "\n",
        "    #Custom sum loss function \n",
        "    \n",
        "    print(\"------------------------ SUM -----------------------\")\n",
        "\n",
        "    sum_save_path = save_path + \"-sum-\"\n",
        "\n",
        "    classifier_sum = RandomForest(min_samples_split, max_depth, features, weights,\n",
        "                                   True, \"custom_1\", n_estimators, model_type=\"class\", max_features=\"auto\")\n",
        "    classifier_sum.fit(df_train, train_frac)\n",
        "\n",
        "    _, sum_preds = classifier_sum.predict_class(df_test)\n",
        "\n",
        "    classifier_sum.get_tree_info(save_path=sum_save_path)\n",
        "    classifier_sum.plot_feature_importance(save_path=sum_save_path)\n",
        "\n",
        "    s_s, b_s, max_sensitivity = _calculate_sensitivity(\n",
        "        sum_preds, df_test['data_type'].to_numpy(), df_test['summed_weight'].to_numpy())\n",
        "\n",
        "    print(\"Signal predicted as Signal\", s_s)\n",
        "    print(\"Background predicted as Signal\", b_s)\n",
        "    print(\"Maximum sensitivity\", max_sensitivity)\n",
        "    print()\n",
        "\n",
        "    df_test['predicted_data_type'] = sum_preds\n",
        "\n",
        "    predicted_as_signal = df_test.loc[df_test['predicted_data_type'] == 1]\n",
        "    predicted_as_background = df_test.loc[df_test['predicted_data_type'] == 0]\n",
        "\n",
        "    for feature in most_important_features:\n",
        "        plot_validation(predicted_as_signal, predicted_as_background, feature, 18e4, 30e4, 300,\n",
        "                        title=\"sum signal prediction\", limit=False, save_path=sum_save_path + feature + \"-signal\")\n",
        "        plot_validation(predicted_as_background, predicted_as_signal, feature, 18e4, 30e4, 300,\n",
        "                        title=\"sum background prediction\", limit=False, save_path=sum_save_path + feature + \"-background\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gI2e4BCmOe-0"
      },
      "source": [
        "### Other helper functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Oujb9FQOe-0"
      },
      "outputs": [],
      "source": [
        "\n",
        "def make_and_evalaute_model_RF(min_samples_split, max_depth, features, weights, use_sum, df_train, df_test, split_type, n_estimators =10, train_frac= 0.1, save_path = \"\", model_type = \"regr\", use_var_red = True, max_features = \"auto\", predict_on_train = False):\n",
        "  print(\"------------------------ RF REGR------------------------\")\n",
        "  \n",
        "  save_path +=  model_type + \"-\"\n",
        "\n",
        "  model = RandomForest(min_samples_split, max_depth, features,weights, use_sum,  split_type, n_estimators, model_type = \"regr\", max_features = max_features)\n",
        "  model.fit(df_train_valid, train_frac)\n",
        "\n",
        "  all_preds= model.predict(df_test)\n",
        "  model.get_tree_info(save_path = save_path)\n",
        "\n",
        "  if predict_on_train:\n",
        "    train_all_preds = model.predict(df_train)\n",
        "    model.get_tree_info(save_path =  save_path + \"train-\")\n",
        "\n",
        "  model.plot_feature_importance(save_path = save_path)\n",
        "\n",
        "  if use_var_red:\n",
        "    print()\n",
        "    print(\"------------------------ RF REGR VAR RED ------------------------\")\n",
        "\n",
        "    regressor_sklearn = RandomForest(min_samples_split, max_depth, features,weights, False, \"var_red\", n_estimators, model_type = model_type, max_features = max_features)\n",
        "    regressor_sklearn.fit(df_train, train_frac)\n",
        "\n",
        "    all_predict_sklearn = regressor_sklearn.predict(df_test)\n",
        "    regressor_sklearn.get_tree_info(save_path =   save_path + \"var-red-\")\n",
        "    regressor_sklearn.plot_feature_importance(save_path =   save_path + \"var-red-\")\n",
        "\n",
        "    var_red_save_path = save_path + \"-var-red\"\n",
        "\n",
        "    \n",
        "    if predict_on_train:\n",
        "      train_all_predict_sklearn = regressor_sklearn.predict(df_train)\n",
        "      regressor_sklearn.get_tree_info(save_path =   save_path + \"train-var-red-\")\n",
        "      print(\"------------------------ TRAIN DATA ------------------------\")\n",
        "      plot_sensitivity(df_train, train_all_preds, train_all_predict_sklearn, max_depth, min_samples_split,  var_red_save_path + \"train-\")\n",
        "    print(\"------------------------ TEST DATA ------------------------\")\n",
        "    return plot_sensitivity(df_test, all_preds, all_predict_sklearn, max_depth, min_samples_split, var_red_save_path)\n",
        "\n",
        "  if train_preds:\n",
        "\n",
        "    print(\"------------------------ TRAIN DATA ------------------------\")\n",
        "    plot_sensitivity(df_train, train_all_preds, np.array([]), max_depth, min_samples_split,  save_path + \"train-\")\n",
        "\n",
        "  print(\"------------------------ TEST DATA ------------------------\")\n",
        "  return plot_sensitivity(df_test, all_preds, np.array([]), max_depth, min_samples_split, save_path)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def make_and_evalaute_model_RF_class(min_samples_split, max_depth, features, weights, use_sum, df_train, df_test, split_type, n_estimators =10, train_frac= 0.1, save_path = \"\", model_type = \"class\", use_var_red = True, weighted = \"both\", max_features = \"auto\", predict_on_train = False):\n",
        "  print(\"------------------------ RF CLASS------------------------\")\n",
        "\n",
        "  save_path +=  model_type + \"-\"\n",
        "\n",
        "\n",
        "  model = RandomForest(min_samples_split = min_samples_split, max_depth= max_depth, features = features,weights = ['summed_weight'], use_sum = use_sum,  split_type = \"custom_1\", n_estimators = n_estimators , model_type = \"class\", max_features = max_features)\n",
        "  model.fit(df_train_valid, train_frac)\n",
        "\n",
        "  all_preds, all_unweighted_preds = model.predict_class(df_test)\n",
        "  model.plot_feature_importance(save_path = save_path)\n",
        "  model.get_tree_info(save_path = save_path)\n",
        "\n",
        "  if predict_on_train:\n",
        "    all_train_preds, all_unweighted_train_preds = model.predict_class(df_train)\n",
        "    model.get_tree_info(save_path = save_path + \"train-\")\n",
        "\n",
        "\n",
        "\n",
        "  if use_var_red:\n",
        "  \n",
        "    regressor_sklearn = RandomForest(min_samples_split = min_samples_split, max_depth= max_depth, features = features,weights = ['summed_weight'], use_sum = False,  split_type = \"var_red\", n_estimators = n_estimators , model_type = \"regr\", max_features = max_features)\n",
        "    regressor_sklearn.fit(df_train, train_frac)\n",
        "\n",
        "    all_predict_sklearn = regressor_sklearn.predict_regr(df_test)\n",
        "    \n",
        "    assess_model_classification(df_test, all_preds, all_predict_sklearn, max_depth, min_samples_split, save_path)\n",
        "  \n",
        "  if weighted == True:\n",
        "    s_s, b_s, max_sensitivity = _calculate_sensitivity(all_preds, df_test['data_type'].to_numpy(), df_test['summed_weight'].to_numpy())\n",
        "\n",
        "    print(\"Signal predicted as Signal\", s_s)\n",
        "    print(\"Background predicted as Signal\", b_s)\n",
        "    print(\"Maximum sensitivity\", max_sensitivity)\n",
        "    print()\n",
        "    return round(max_sensitivity,3)\n",
        "    \n",
        "  elif weighted == \"both\":\n",
        "\n",
        "    s_s, b_s, max_sensitivity = _calculate_sensitivity(all_preds, df_test['data_type'].to_numpy(), df_test['summed_weight'].to_numpy())\n",
        "    u_s_s, u_b_s, u_max_sensitivity = _calculate_sensitivity(all_unweighted_preds, df_test['data_type'].to_numpy(), df_test['summed_weight'].to_numpy())\n",
        "\n",
        "\n",
        "\n",
        "    print(\"Signal predicted as Signal\", s_s)\n",
        "    print(\"Background predicted as Signal\", b_s)\n",
        "    print(\"Maximum sensitivity\", max_sensitivity)\n",
        "    print()\n",
        "\n",
        "    print(\"Unweighted Signal predicted as Signal\", u_s_s)\n",
        "    print(\"Unweighted Background predicted as Signal\", u_b_s)\n",
        "    print(\"Unweighted Maximum sensitivity\", u_max_sensitivity)\n",
        "    print()\n",
        "\n",
        "    if predict_on_train:\n",
        "      train_s_s, train_b_s, train_max_sensitivity = _calculate_sensitivity(all_train_preds, df_train['data_type'].to_numpy(), df_train['summed_weight'].to_numpy())\n",
        "      train_u_s_s, train_u_b_s, train_u_max_sensitivity = _calculate_sensitivity(all_unweighted_train_preds, df_train['data_type'].to_numpy(), df_train['summed_weight'].to_numpy())\n",
        "\n",
        "      print(\"Train Signal predicted as Signal\", train_s_s)\n",
        "      print(\"Train Background predicted as Signal\", train_b_s)\n",
        "      print(\"Train Maximum sensitivity\", train_max_sensitivity)\n",
        "      print()\n",
        "\n",
        "      print(\"Train Unweighted Signal predicted as Signal\", train_u_s_s)\n",
        "      print(\"Train Unweighted Background predicted as Signal\", train_u_b_s)\n",
        "      print(\"Train Unweighted Maximum sensitivity\", train_u_max_sensitivity)\n",
        "      print()\n",
        "    return round(max_sensitivity, 3), round(u_max_sensitivity,3)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihmCihqaOe-2"
      },
      "source": [
        "### implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frOMJhZoOe-2"
      },
      "outputs": [],
      "source": [
        "\n",
        "class RandomForest():\n",
        "  def __init__(self, min_samples_split=1000, max_depth=2, features = [], weights = [], use_sum= False,  split_type = \"\", n_estimators = 20, max_features = \"auto\", model_type = \"regr\"):\n",
        "      self.min_samples_split = min_samples_split\n",
        "      self.max_depth = max_depth\n",
        "      self.features = features\n",
        "      self.use_sum = use_sum\n",
        "      self.s_s = 0\n",
        "      self.s_b = 0\n",
        "      self.n_estimators = n_estimators\n",
        "      self.tree_models = []\n",
        "      self.model_sensitivities = []\n",
        "      self.model_features = []\n",
        "      self.weights = weights\n",
        "      self.split_type = split_type\n",
        "      self.model_type = model_type\n",
        "      self.max_features = max_features\n",
        "\n",
        "  def plot_feature_importance(self, save_path):\n",
        "    feature_importance = self.get_cumulative_feature_importance()\n",
        "\n",
        "    features_sorted =  dict(sorted(feature_importance.items(), key=lambda item: item[1], reverse = True))\n",
        "    plt.clf()\n",
        "    plt.title(\"Random forest feature importance\" )\n",
        "\n",
        "    plt.bar(range(len(features_sorted)), list(features_sorted.values()), align='center')\n",
        "    plt.xticks(range(len(features_sorted)), list(features_sorted.keys()))\n",
        "\n",
        "    N = len(self.features)\n",
        "    plt.xticks(range(N)) \n",
        "    plt.grid()\n",
        "\n",
        "    plt.gca().margins(x=0)\n",
        "    plt.gcf().canvas.draw()\n",
        "    tl = plt.gca().get_xticklabels()\n",
        "    maxsize = max([t.get_window_extent().width for t in tl])\n",
        "    m = 1.0 \n",
        "    s = maxsize/plt.gcf().dpi*N+2*m\n",
        "    margin = m/plt.gcf().get_size_inches()[0]\n",
        "\n",
        "    plt.gcf().subplots_adjust(left=margin, right=1.-margin)\n",
        "    plt.gcf().set_size_inches(s, plt.gcf().get_size_inches()[1])\n",
        "\n",
        "    if save_path:\n",
        "      final_save_path = save_path + \"feature-importance.png\"\n",
        "      plt.savefig(final_save_path) \n",
        "\n",
        "    plt.show()\n",
        "  \n",
        "\n",
        "  def get_tree_info(self, save_path = \"\"):\n",
        "    with_sensitivities = True if self.split_type == \"custom_1\" and self.model_type != \"regr\" else False\n",
        "    \n",
        "    if with_sensitivities:\n",
        "      lowest_sens = 999\n",
        "      lowest_index = 0\n",
        "      highest_sens = -999\n",
        "      highest_index = 0\n",
        "\n",
        "      output_text = \"\"\n",
        "\n",
        "      for i in range(0, len(self.model_sensitivities)):\n",
        "        if (self.model_sensitivities[i] < lowest_sens):\n",
        "          lowest_index = i\n",
        "          lowest_sens = self.model_sensitivities[i]\n",
        "\n",
        "        if (self.model_sensitivities[i] > highest_sens):\n",
        "          highest_index = i\n",
        "          highest_sens = self.model_sensitivities[i]\n",
        "\n",
        "        temp = \"Tree \" + str(i) + \": \" + \"\\n\" + \"Sens: \" + str(self.model_sensitivities[i]) + \", features: \" + \"[\" + ' '.join(self.model_features[i])+ \"]\" + \"\\n\"\n",
        "        output_text += temp\n",
        "\n",
        "      temp = \"Highest sens tree: \" + \"\\n\" + \"Sens: \" + str(self.model_sensitivities[highest_index]) + \", features: \" + \"[\" + ' '.join(self.model_features[highest_index])+ \"]\" + \"\\n\"\n",
        "      output_text += temp\n",
        "\n",
        "      temp = \"Lowest sens tree: \" + \"\\n\" + \"Sens: \" + str(self.model_sensitivities[lowest_index]) + \", features: \" + \"[\" + ' '.join(self.model_features[lowest_index])+ \"]\" + \"\\n\"\n",
        "      output_text += temp\n",
        "\n",
        "    else:\n",
        "      output_text = \"\"\n",
        "\n",
        "      for i in range(0, len(self.model_features)):\n",
        "        temp = \"Tree \" + str(i) + \": \" + \"\\n\" +  \", features: \" + \"[\" + ' '.join(self.model_features[i])+ \"]\" + \"\\n\"\n",
        "        output_text += temp\n",
        "\n",
        "    if save_path:\n",
        "      final_save_path = save_path + \"individual_trees.txt\" \n",
        "      f= open(final_save_path,\"w+\")\n",
        "      f.write(output_text)\n",
        "      f.close()\n",
        "        \n",
        "    return output_text\n",
        "\n",
        "\n",
        "\n",
        "  def get_trees(self):\n",
        "    return self.tree_models\n",
        "\n",
        "  def get_cumulative_feature_importance(self):\n",
        "    feature_importance_list = []\n",
        "    for tree in self.tree_models:\n",
        "      feature_importance_list.append(Counter(tree.get_feature_importance_dict()))\n",
        "    cumulative_feature_importance = sum(feature_importance_list, Counter())\n",
        "    relative_cumulative_feature_importance = {k:v/self.n_estimators for k,v in cumulative_feature_importance.items()}\n",
        "\n",
        "\n",
        "    for feature in relative_cumulative_feature_importance.keys():\n",
        "      feature_name = feature.replace(\"\\n\", \"_\")\n",
        "      relative_cumulative_feature_importance[feature] = relative_cumulative_feature_importance[feature] / sum(x.count(feature_name) for x in self.model_features)\n",
        "\n",
        "    return relative_cumulative_feature_importance\n",
        "\n",
        "    \n",
        "  def get_weigthed_data_sample(self, dataset, frac, background_count):\n",
        "      signal_data = dataset.loc[dataset['data_type'] == 1]\n",
        "\n",
        "      data_sample = signal_data.sample(frac = frac, replace = True)\n",
        "\n",
        "      # Append a proportional amount of each background to the data_sample\n",
        "\n",
        "      for i in range(0, background_count):\n",
        "          #background is classified incrementally from 2\n",
        "          background_id = i+2\n",
        "          bg_sample = dataset.loc[dataset['full_data_type'] == background_id].sample(frac = frac, replace = True)\n",
        "          data_sample = data_sample.append(bg_sample)\n",
        "          \n",
        "      return shuffle(data_sample)\n",
        "      \n",
        "\n",
        "  def fit(self, data, single_tree_data_frac):\n",
        "    for i in range (0,self.n_estimators):\n",
        "      if self.max_features == \"auto\":\n",
        "        selected_features = random.sample(features, math.ceil(math.sqrt(len(features))))\n",
        "      else:\n",
        "        selected_features = random.sample(features, self.max_features)\n",
        "      data_sample = self.get_weigthed_data_sample(data, single_tree_data_frac, 11) # 11 = number of different data types\n",
        "      model = DecisionTreeRegressor4(self.min_samples_split, self.max_depth, selected_features, self.weights, self.use_sum, self.split_type, self.model_type)\n",
        "      model.fit(data_sample)\n",
        "      self.model_features.append(selected_features)\n",
        "      self.tree_models.append(model)\n",
        "\n",
        "\n",
        "  def predict(self, data):\n",
        "    if self.model_type == \"regr\":\n",
        "      return self.predict_regr(data)\n",
        "    else:\n",
        "      return self.predict_class(data)\n",
        "\n",
        "  def predict_regr(self, data):\n",
        "    predictions = []\n",
        "    \n",
        "    for model in self.tree_models:\n",
        "      tree_preds = model.predict(data[self.features])\n",
        "      _, _, tree_sensitivity = _calculate_sensitivity(tree_preds, data['data_type'].to_numpy(), data['summed_weight'].to_numpy())\n",
        "      self.model_sensitivities.append(tree_sensitivity)\n",
        "      predictions.append(tree_preds)\n",
        "      \n",
        "    return np.average(predictions, axis=0)\n",
        "\n",
        "\n",
        "  def predict_class(self, data):\n",
        "    self.model_sensitivities = []\n",
        "    predictions = []\n",
        "    sensitivities = [] \n",
        "    for model in self.tree_models:\n",
        "      if self.use_sum:\n",
        "        preds = model.predict(data[self.features])\n",
        "        _, _, tree_sensitivity = _calculate_sensitivity(preds, data['data_type'].to_numpy(), data['summed_weight'].to_numpy())\n",
        "        sensitivities.append([tree_sensitivity] * len(preds))\n",
        "        self.model_sensitivities.append(tree_sensitivity)\n",
        "        predictions.append(preds)\n",
        "\n",
        "      else:\n",
        "        preds, sens = model.predict(data[self.features], True)\n",
        "        sensitivities.append(sens)\n",
        "        predictions.append(preds)\n",
        "      \n",
        "  \n",
        "    weighted_average_prediction = np.average(predictions, axis=0, weights = sensitivities) # if 0.5, then n_signal_preds == n_background_preds\n",
        "    unweighted_average_prediction = np.average(predictions, axis=0) # if 0.5, then n_signal_preds == n_background_preds\n",
        "\n",
        "    weigted_classifications = [1 if x >0.5 else 0 for x in weighted_average_prediction]\n",
        "    unweigted_classifications = [1 if x >0.5 else 0 for x in unweighted_average_prediction]\n",
        "\n",
        "   return weigted_classifications, unweigted_classifications\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YHV8s2h6TmB"
      },
      "source": [
        "## Functions used for sensitivity plots and other performance assessment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiuOih8GOe_Y"
      },
      "outputs": [],
      "source": [
        " def plot_fluctuations(runs_weighted, runs_unweighted): \n",
        "    x = []\n",
        "    for i in range(0, len(runs_unweighted)):\n",
        "        x.append(i)\n",
        "    \n",
        "    \n",
        "    if runs_weighted:\n",
        "      min_value = min(runs_weighted + runs_unweighted)\n",
        "      max_value = max(runs_weighted + runs_unweighted)\n",
        "    else:\n",
        "      min_value = min(runs_unweighted)\n",
        "      max_value = max(runs_unweighted)\n",
        "\n",
        "    plt.clf()\n",
        "    plt.figure(figsize=(12,8))\n",
        "    plt.ylim(min_value - min_value * 0.3, max_value + max_value * 0.3)\n",
        "    \n",
        "\n",
        "    if runs_weighted:\n",
        "      plt.plot(x, runs_weighted, label = \"Custom\")\n",
        "      plt.plot(x, runs_weighted, 'bo')\n",
        "      plt.plot(x, len(runs_weighted)*[np.mean(runs_weighted)], '--', color= \"blue\", alpha =0.25, label = \"Mean custom\")\n",
        "\n",
        "    plt.plot(x, runs_unweighted, label = \"Custom unweighted\")\n",
        "    plt.plot(x, runs_unweighted, 'bo')\n",
        "    plt.plot(x, len(runs_unweighted)*[np.mean(runs_unweighted)], '--', color= \"orange\", alpha =0.25, label = \"Mean custom, unweighted\")\n",
        "\n",
        "\n",
        "    plt.ylabel(\"Peak Sensitivity\")\n",
        "    plt.xlabel(\"nth run\")\n",
        "    plt.title(\"Model's fluctuations using same hyperparameters \\n All dataset, 0.25 train_pct, 30 estimators, weighted RF\")\n",
        "    plt.legend(loc=\"upper right\")\n",
        "    plt.xticks(x)\n",
        "\n",
        "    if runs_weighted:\n",
        "        \n",
        "      for i_x, i_y in zip(x, runs_weighted):\n",
        "          plt.text(i_x, i_y +0.1, '{}'.format(i_y))\n",
        "\n",
        "    for i_x, i_y in zip(x, runs_unweighted):\n",
        "        plt.text(i_x, i_y - 0.1, '{}'.format(i_y))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSL2m3L7Oe_Z"
      },
      "outputs": [],
      "source": [
        "\n",
        "def assess_model_classification(data, preds, var_red_preds, depth, min_split, save_path = \"\"):\n",
        "  use_var_red = True if var_red_preds.size else False\n",
        "\n",
        "  s_s, b_s, max_sensitivity = _calculate_sensitivity(preds, data['data_type'].to_numpy(), data['summed_weight'].to_numpy())\n",
        "  thresholds_vr, sensitivity_measures_vr, max_sensitivity_vr, counts_vr, _, _ = get_sensitivity_values(data, var_red_preds, True)\n",
        "\n",
        "\n",
        "  rows = [\"Custom Test\", \"SK Test\"]\n",
        "  columns = [\"Signal as Signal\", \"Signal as Background\", \"Background as Background\", \"Background as Signal\"]\n",
        "  data = {\n",
        "      'w_s_s' : [round(s_s,0), counts_vr[0]],\n",
        "      'w_b_s' : [round(b_s,0), counts_vr[3]],\n",
        "      'max_sens' : [max_sensitivity, max_sensitivity_vr]\n",
        "  }\n",
        "\n",
        "  df = pd.DataFrame(data=data)\n",
        "\n",
        "  plt.clf()\n",
        "  plt.figure(figsize=(12,8))\n",
        "  ylabel = \"s/\" +  u\"\\u221A\" + \"(s+b)\"\n",
        "  plt.ylabel(ylabel)\n",
        "  plt.xlabel(\"Model Output\")\n",
        "  plt.title(\"Custom Maximum Sensitivity: \" +  str(round(max_sensitivity, 3))  + \"\\nVar-Red Maximum Sensitivity: \" +  str(round(max_sensitivity_vr, 3))  + \"\\nDepth: \" + str(depth) + \" Min_split: \" + str(min_split) )\n",
        "  plt.plot(thresholds_vr, sensitivity_measures_vr, label=\"Variance Reduction\")\n",
        "  plt.legend(loc=\"upper right\")\n",
        "\n",
        "  if save_path != \"\":\n",
        "    plt.savefig(save_path+ \".png\")\n",
        "  plt.show()\n",
        "\n",
        "  plt.figure(figsize=(12,6))\n",
        "  ax = plt.gca()\n",
        "  ax.get_xaxis().set_visible(False)\n",
        "  ax.get_yaxis().set_visible(False)\n",
        "  plt.box(on=None)\n",
        "  the_table = plt.table(cellText=df.values, colLabels=df.columns,rowLabels=rows, loc='center')\n",
        "  the_table.scale(1, 1.5)\n",
        "  plt.subplots_adjust(left=0.2, bottom=0.4)\n",
        "  if save_path != \"\":\n",
        "    table_save_path = save_path + \"-table.png\"\n",
        "    plt.savefig(table_save_path)\n",
        "  plt.show()\n",
        "  plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4jUyp6xOe_Z"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_sensitivity(data, preds, var_red_preds, depth, min_split, save_path = \"\"):\n",
        "  use_var_red = True if var_red_preds.size else False\n",
        "  use_custom = True if preds.size else False\n",
        "  use_both = True if use_var_red and use_custom else False\n",
        "\n",
        "  if use_custom:\n",
        "    thresholds, sensitivity_measures, max_sensitivity, counts, s_s_list, b_s_list, max_sensitivity_threshold = get_sensitivity_values(data, preds, True)\n",
        "\n",
        "  if use_var_red:\n",
        "    thresholds_vr, sensitivity_measures_vr, max_sensitivity_vr, counts_vr, s_s_list_vr, b_s_list_vr, max_sensitivity_threshold = get_sensitivity_values(data, var_red_preds, True)\n",
        "\n",
        "  if use_both:\n",
        "    rows = [\"Custom Test\", \"SK Test\"]\n",
        "    columns = [\"Signal as Signal\", \"Signal as Background\", \"Background as Background\", \"Background as Signal\"]\n",
        "    table_data = {\n",
        "        'w_s_s' : [counts[0], counts_vr[0]],\n",
        "        'w_s_b' : [counts[1], counts_vr[1]],\n",
        "        'w_b_b' : [counts[2], counts_vr[2]],\n",
        "        'w_b_s' : [counts[3], counts_vr[3]],\n",
        "        's_s'   : [counts[4], counts_vr[4]],\n",
        "        's_b'   : [counts[5], counts_vr[5]],\n",
        "        'b_b'   : [counts[6], counts_vr[6]],\n",
        "        'b_s'   : [counts[7], counts_vr[7]],\n",
        "        'max_sens' : [max_sensitivity, max_sensitivity_vr]\n",
        "    }\n",
        "  elif use_custom:\n",
        "    rows = [\"Custom Test\"]\n",
        "    columns = [\"Signal as Signal\", \"Signal as Background\", \"Background as Background\", \"Background as Signal\"]\n",
        "    table_data = {\n",
        "        'w_s_s' : [counts[0]],\n",
        "        'w_s_b' : [counts[1]],\n",
        "        'w_b_b' : [counts[2]],\n",
        "        'w_b_s' : [counts[3]],\n",
        "        's_s'   : [counts[4]],\n",
        "        's_b'   : [counts[5]],\n",
        "        'b_b'   : [counts[6]],\n",
        "        'b_s'   : [counts[7]],\n",
        "        'max_sens' : [max_sensitivity]\n",
        "    }\n",
        "  else:\n",
        "    rows = [\"Custom Test\"]\n",
        "    columns = [\"Signal as Signal\", \"Signal as Background\", \"Background as Background\", \"Background as Signal\"]\n",
        "    table_data = {\n",
        "        'w_s_s' : [counts_vr[0]],\n",
        "        'w_s_b' : [counts_vr[1]],\n",
        "        'w_b_b' : [counts_vr[2]],\n",
        "        'w_b_s' : [counts_vr[3]],\n",
        "        's_s'   : [counts_vr[4]],\n",
        "        's_b'   : [counts_vr[5]],\n",
        "        'b_b'   : [counts_vr[6]],\n",
        "        'b_s'   : [counts_vr[7]],\n",
        "        'max_sens' : [max_sensitivity_vr]\n",
        "    }\n",
        "\n",
        "\n",
        "  df = pd.DataFrame(data=table_data)\n",
        "  plt.clf()\n",
        "  plt.figure(figsize=(12,8))\n",
        "  ylabel = \"s/\" +  u\"\\u221A\" + \"(s+b)\"\n",
        "  plt.ylabel(ylabel)\n",
        "  plt.xlabel(\"Model Output\")\n",
        "\n",
        "  if use_both:\n",
        "    plt.title(\"Custom Maximum Sensitivity: \" +  str(round(max_sensitivity, 3))  + \"\\nVar-Red Maximum Sensitivity: \" +  str(round(max_sensitivity_vr, 3))  + \"\\nDepth: \" + str(depth) + \" Min_split: \" + str(min_split) + \"\\n Best threshold:\" + str(max_sensitivity_threshold) )\n",
        "    plt.plot(thresholds_vr, sensitivity_measures_vr, label=\"Var-Red Test Data\")\n",
        "    plt.plot(thresholds, sensitivity_measures, label=\"Custom Test Data\")\n",
        "    \n",
        "  elif use_custom:\n",
        "    plt.title(\"Custom Maximum Sensitivity: \" +  str(round(max_sensitivity, 3))  + \"\\nDepth: \" + str(depth) + \" Min_split: \" + str(min_split) +\"\\n Best threshold:\" + str(max_sensitivity_threshold) )\n",
        "    plt.plot(thresholds, sensitivity_measures, label=\"Custom Test Data\")\n",
        "\n",
        "  else:\n",
        "    plt.title(\"Variance Reduction Maximum Sensitivity: \" +  str(round(max_sensitivity_vr, 3))  + \"\\nDepth: \" + str(depth) + \" Min_split: \" + str(min_split) + \"\\n Best threshold:\" + str(max_sensitivity_threshold))\n",
        "    plt.plot(thresholds_vr, sensitivity_measures_vr, label=\"Var-Red Test Data\")\n",
        "\n",
        "    \n",
        "\n",
        "  plt.legend(loc=\"upper right\")\n",
        "\n",
        "  if save_path != \"\":\n",
        "    plt.savefig(save_path+ \".png\")\n",
        "  plt.show()\n",
        "\n",
        "  plt.figure(figsize=(12,2))\n",
        "  ax = plt.gca()\n",
        "  ax.get_xaxis().set_visible(False)\n",
        "  ax.get_yaxis().set_visible(False)\n",
        "  plt.box(on=None)\n",
        "  the_table = plt.table(cellText=df.values, colLabels=df.columns,rowLabels=rows, loc='center')\n",
        "  the_table.scale(1, 1.5)\n",
        "  plt.subplots_adjust(left=0.2, bottom=0.4)\n",
        "  if save_path != \"\":\n",
        "    table_save_path = save_path + \"-table.png\"\n",
        "    plt.savefig(table_save_path)\n",
        "  plt.show()\n",
        "\n",
        "  if use_both: return round(max_sensitivity_threshold, 3), round(max_sensitivity_threshold, 3)\n",
        "  elif use_var_red: return round(max_sensitivity_threshold, 3)\n",
        "  return round(max_sensitivity_threshold, 3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-s0vdrJOe_Z"
      },
      "outputs": [],
      "source": [
        "def get_sensitivity_values(data, preds, return_counts = False):\n",
        "  # compute predictions, sensitivies, true positive and false positive rates\n",
        "\n",
        "  data['prediction'] = np.array(preds)\n",
        "\n",
        "  signal_data = data.loc[data['data_type'] == 1]\n",
        "  background_data = data.loc[data['data_type'] == 0]\n",
        "\n",
        "  sensitivity_measures = []\n",
        "  s_s_list = []\n",
        "  b_s_list = []\n",
        "\n",
        "  max_sensitivity, s_s, s_b, b_b, b_s, total, max_sensitivity_threshold = 0,0,0,0,0,0,0\n",
        "  unweighted_s_s, unweighted_s_b, unweighted_b_b, unweighted_b_s = 0,0,0,0\n",
        "\n",
        "  thresholds = np.arange(0.0, 1, 0.025)\n",
        "\n",
        "\n",
        "\n",
        "  for threshold in thresholds:\n",
        "    unweighted_signal_predicted_as_signal = signal_data.loc[signal_data[\"prediction\"] > threshold].shape[0]\n",
        "    unweighted_signal_predicted_as_background = signal_data.loc[signal_data[\"prediction\"] <= threshold].shape[0]\n",
        "\n",
        "    unweighted_background_predicted_as_background = background_data.loc[background_data[\"prediction\"] <= threshold].shape[0]\n",
        "    unweighted_background_predicted_as_signal = background_data.loc[background_data[\"prediction\"] > threshold].shape[0]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    signal_predicted_as_signal = signal_data.loc[signal_data[\"prediction\"] > threshold]['summed_weight'].sum()\n",
        "    signal_predicted_as_background = signal_data.loc[signal_data[\"prediction\"] <= threshold]['summed_weight'].sum()\n",
        "\n",
        "    background_predicted_as_background = background_data.loc[background_data[\"prediction\"] <= threshold]['summed_weight'].sum()\n",
        "    background_predicted_as_signal = background_data.loc[background_data[\"prediction\"] > threshold]['summed_weight'].sum()\n",
        "\n",
        "    s_s_list.append(signal_predicted_as_signal)\n",
        "    b_s_list.append(background_predicted_as_signal)\n",
        "\n",
        "\n",
        "    sensitivity = 0\n",
        "\n",
        "    if( (signal_predicted_as_signal + background_predicted_as_signal) > 0):\n",
        "      sensitivity = signal_predicted_as_signal / math.sqrt((signal_predicted_as_signal + background_predicted_as_signal))\n",
        "\n",
        "    sensitivity_measures.append(sensitivity)\n",
        "\n",
        "    if sensitivity > max_sensitivity:\n",
        "      max_sensitivity = sensitivity\n",
        "      s_s = signal_predicted_as_signal\n",
        "      s_b = signal_predicted_as_background\n",
        "      b_b = background_predicted_as_background\n",
        "      b_s = background_predicted_as_signal\n",
        "\n",
        "      unweighted_s_s = unweighted_signal_predicted_as_signal\n",
        "      unweighted_s_b = unweighted_signal_predicted_as_background\n",
        "      unweighted_b_b = unweighted_background_predicted_as_background\n",
        "      unweighted_b_s = unweighted_background_predicted_as_signal\n",
        "      total = s_s + s_b + b_b + b_s\n",
        "      max_sensitivity_threshold = threshold\n",
        "\n",
        "  counts = [round(s_s,0), round(s_b,0), round(b_b,0), round(b_s,0), round(unweighted_s_s,0), round(unweighted_s_b,0), round(unweighted_b_b,0), round(unweighted_b_s,0) ]\n",
        "  if return_counts:\n",
        "    return thresholds, sensitivity_measures, round(max_sensitivity,3),counts, s_s_list, b_s_list, round(max_sensitivity_threshold, 4)\n",
        "  \n",
        "  return thresholds, sensitivity_measures, round(max_sensitivity,3), counts, round(max_sensitivity_threshold,4)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0V1FKnGOe_a"
      },
      "source": [
        "## Custom split selection testing "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXfyuQbdOe_a"
      },
      "outputs": [],
      "source": [
        "@jit(nopython= True, fastmath=True, parallel= True)\n",
        "def _split_GPU(r_child, r_child_weights, l_child, l_child_weights):\n",
        "\n",
        "  parent = np.append(l_child, r_child, axis=0)\n",
        "  parent_weights = np.append(l_child_weights, r_child_weights, axis=0)\n",
        "\n",
        "  parent_background_count = 0\n",
        "  parent_signal_count = 0\n",
        "\n",
        "\n",
        "  for i in range(len(parent)):\n",
        "    if parent[i] == 1:\n",
        "      parent_signal_count += parent_weights[i]\n",
        "    else:\n",
        "      parent_background_count += parent_weights[i]\n",
        "\n",
        "\n",
        "  r_child_background_count = 0\n",
        "  r_child_signal_count = 0\n",
        "\n",
        "  for i in range(len(r_child)):\n",
        "    if r_child[i] == 1:\n",
        "      r_child_signal_count += r_child_weights[i]\n",
        "    else:\n",
        "      r_child_background_count += r_child_weights[i]\n",
        "\n",
        "  measure_child = r_child_signal_count / math.sqrt( r_child_signal_count + r_child_background_count) \n",
        "\n",
        "  measure_parent = parent_signal_count / math.sqrt( parent_signal_count + parent_background_count)\n",
        "  reduction =  measure_child - measure_parent\n",
        "\n",
        "\n",
        "  return reduction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nddcdeEOe_a"
      },
      "outputs": [],
      "source": [
        "def get_reduced_threshold_list(possible_thresholds, n = 10):\n",
        "  try:\n",
        "    return possible_thresholds[::int((len(possible_thresholds)/n))]\n",
        "  except:\n",
        "    return possible_thresholds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkKgqJ5jOe_a"
      },
      "outputs": [],
      "source": [
        "def get_split_dict(n = 10):\n",
        "  measure_dict = {}\n",
        "  for feature in features: \n",
        "    measures =[]\n",
        "    # print(feature)\n",
        "    thresholds = np.sort(np.unique(df_train_valid[feature].round(decimals=3))) # thresholds\n",
        "    if n != \"unlimited\":\n",
        "      thresholds = get_reduced_threshold_list(thresholds, n = n)\n",
        "\n",
        "    final_thresholds = []\n",
        "    for threshold in thresholds:\n",
        "      r_child =  df_train_valid[df_train_valid[feature] > threshold]\n",
        "      if (r_child['summed_weight'].sum()> 0):\n",
        "        measure= _split_GPU(r_child['data_type'].to_numpy(), r_child['summed_weight'].to_numpy(), df_train_valid['data_type'].to_numpy(), df_train_valid['summed_weight'].to_numpy())\n",
        "        measures.append(measure)\n",
        "        final_thresholds.append(threshold)\n",
        "    measure_dict[feature] = [final_thresholds, measures]\n",
        "  return measure_dict\n",
        "\n",
        "\n",
        "def plot_split_dict(save_path = \"\"):\n",
        "  for feature in features:\n",
        "    print(feature, max(split_dict[feature][1]))\n",
        "    thresholds= split_dict[feature][0]\n",
        "    measures = split_dict[feature][1]\n",
        "    plt.figure(figsize=(12,8))\n",
        "\n",
        "    plt.title(feature)\n",
        "    plt.plot(thresholds, measures)\n",
        "    if save_path != \"\":\n",
        "      path = save_path + feature +\".png\"\n",
        "      plt.savefig(path)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3o3TDmQAOe_b"
      },
      "outputs": [],
      "source": [
        "for n in [10, 50, 100]:\n",
        "    save_path = \"feature_split_validation/\" + str(n)\n",
        "    split_dict = get_split_dict(n = n)\n",
        "    plot_split_dict(save_path = save_path)\n",
        "\n",
        "save_path = \"feature_split_validation/unlimited\"\n",
        "split_dict = get_split_dict(n = n)\n",
        "plot_split_dict(save_path = save_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZNb-Abvsz_o"
      },
      "source": [
        "# Plotting data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gm2eMyVh1cGq"
      },
      "outputs": [],
      "source": [
        "def plot_multiple(data_set, label_set):\n",
        "  for i in range (0, len(data_set)):\n",
        "    data = data_set[i]\n",
        "    label = label_set[i]\n",
        "    plt.hist(data, label=label)\n",
        "    plt.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
        "    plt.legend(loc=\"upper left\")\n",
        "  plt.show()\n",
        "\n",
        "def plot_single(label):\n",
        "  data = signal[label]\n",
        "  plt.hist(data, label=label, bins=50)\n",
        "  plt.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
        "  plt.legend(loc=\"upper left\")\n",
        "  plt.show()\n",
        "\n",
        "def plot_signal_and_background(label):\n",
        "  background_data = [item[label] for item in background]\n",
        "  signal_data = signal[label]\n",
        "  \n",
        "  plt.rcParams['figure.figsize'] = [16, 9]\n",
        "  plt.xlabel(label)\n",
        "  plt.ylabel(\"Events per bin\")\n",
        "  plt.hist( background_data, bins = 100, stacked=True, label=backgroundLabel )\n",
        "  # plt.hist(signal_data, bins=100, color='black', histtype=u'step', label='signal')\n",
        "  plt.legend(loc='best', prop={'size': 7})\n",
        "  plt.yscale('log')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def scatter(x_label, y_label):\n",
        "  signal_x_data, signal_y_data = signal[x_label], signal[y_label]\n",
        "  background_x_data = [item[x_label] for item in background]\n",
        "  background_y_data = [item[y_label] for item in background]\n",
        "\n",
        "  plt.rcParams['figure.figsize'] = [16, 9]\n",
        "  plt.rcParams[\"legend.markerscale\"] = 3\n",
        "  plt.xlabel(x_label)\n",
        "  plt.ylabel(y_label)\n",
        "\n",
        "\n",
        "  for i in range(0,len(background)):\n",
        "    plt.scatter(background[i][x_label], background[i][y_label], s = 0.7, label = backgroundLabel[i])\n",
        "\n",
        "  plt.scatter(signal_x_data, signal_y_data, s=1.2, color='black', label = 'signal')\n",
        "\n",
        "  plt.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
        "  plt.legend(loc='best', prop={'size': 12})\n",
        "  plt.show()\n",
        "\n",
        "def plot_subtract(label_1, label_2):\n",
        "  \n",
        "  background_data = [item[label_1] - item[label_2] for item in background]\n",
        "  signal_data = signal[label_1] - signal[label_2]\n",
        "\n",
        "  plt.rcParams['figure.figsize'] = [16, 9]\n",
        "  plt.xlabel(label_1 + \"-\" + label_2)\n",
        "  plt.ylabel(\"Events per bin\")\n",
        "  plt.hist( background_data, bins = 50, stacked=True, label=backgroundLabel )\n",
        "  plt.hist(signal_data, bins=50, color='black', histtype=u'step', label='signal')\n",
        "  plt.legend(loc='best', prop={'size': 12})\n",
        "  # plt.yscale('log')\n",
        "  plt.show()\n",
        "\n",
        "def plot_ttbar(label, save_path = \"\", norm=False):\n",
        "  if norm:\n",
        "    signal_data =  signal_norm[label]\n",
        "    background_data = ttbar_norm[label]\n",
        "  else:\n",
        "    signal_data =  signal[label]\n",
        "    background_data = ttbar[label]\n",
        "\n",
        "\n",
        "\n",
        "  b_weights = ttbar['lumiweight'] *  ttbar['mcEventWeight']  * ttbar['pileupweight']\n",
        "  s_weights = signal['lumiweight'] *  signal['mcEventWeight']  * signal['pileupweight'] \n",
        "\n",
        "  if norm:\n",
        "    b_weights = ttbar_norm['lumiweight'] *  ttbar_norm['mcEventWeight']  * ttbar_norm['pileupweight']\n",
        "    s_weights = signal_norm['lumiweight'] *  signal_norm['mcEventWeight']  * signal_norm['pileupweight'] \n",
        "\n",
        "\n",
        "  plt.rcParams['figure.figsize'] = [16, 9]\n",
        "  plt.yscale('log')\n",
        "  plt.xticks(fontsize=14)\n",
        "  plt.yticks(fontsize=14)\n",
        "  if norm:\n",
        "    _, bins, p = plt.hist( background_data, bins = 50, label=backgroundLabel, weights = b_weights, color = \"#003f5c\", density=True)\n",
        "    plt.hist(signal_data, bins=bins, color='#ffa600', label='signal (wtaunu)', weights = s_weights,  histtype=u'step', density=True)  \n",
        "  else:\n",
        "    _, bins, p = plt.hist( background_data, bins = 50, label=backgroundLabel, weights = b_weights, color = \"#003f5c\", density=True)\n",
        "    plt.hist(signal_data, bins=bins, color='#ffa600', label='signal (wtaunu)', weights = s_weights,  histtype=u'step', density=True) \n",
        "     \n",
        "  plt.legend(loc='best', prop={'size': 15})\n",
        "  if label in mev_features: \n",
        "    plt.gca().set_xlabel(label + \" [MeV]\" , fontsize=18, labelpad=20)\n",
        "  else: \n",
        "    plt.gca().set_xlabel(label , fontsize=18, labelpad=20) \n",
        "\n",
        "\n",
        "  if save_path:\n",
        "    plt.savefig(save_path + label + \".png\")\n",
        "  plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "6g4jEw9XmBGt",
        "HjgeJQj9xjDC"
      ],
      "name": "Masters - latest.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "c5839256469a881d27d5abe2fb0027d5b08f471b9d72df50cf0477efcf897072"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit ('venv': venv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}